{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN/L+CyGtiahzDyHpQ6BDTk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Danzigerrr/MultiClass-Entity-Linking-System/blob/NER-datasets/NED_simple_NED_with_DBpedia_Wikidata.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests"
      ],
      "metadata": {
        "id": "IoTmgZvEqPzH"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZNLPsPWkJh7X"
      },
      "outputs": [],
      "source": [
        "# NER output from Flair (mocked here for demonstration purposes)\n",
        "sentence = \"Notre Dame, the iconic medieval cathedral in Paris, reopens after five years of speedy reconstruction work.\"\n",
        "ner_spans = [\n",
        "    {\"text\": \"Notre Dame\", \"type\": \"FAC\", \"score\": 1.0000},\n",
        "    {\"text\": \"Paris\", \"type\": \"GPE\", \"score\": 1.0000},\n",
        "    {\"text\": \"five years\", \"type\": \"DATE\", \"score\": 1.0000},\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DBpedia\n"
      ],
      "metadata": {
        "id": "9x2k4OdGvzlr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ontonotes_to_dbpedia_mapping = [\n",
        "    {\"OntoNotes\": \"CARDINAL\", \"DBpedia\": [\"Identifier\"]},\n",
        "    {\"OntoNotes\": \"DATE\", \"DBpedia\": [\"TimePeriod\"]},\n",
        "    {\"OntoNotes\": \"EVENT\", \"DBpedia\": [\"Event\", \"Activity\"]},\n",
        "    {\"OntoNotes\": \"FAC\", \"DBpedia\": [\"ArchitecturalStructure\"]},\n",
        "    {\"OntoNotes\": \"GPE\", \"DBpedia\": [\"Place\", \"EthnicGroup\"]},\n",
        "    {\"OntoNotes\": \"LANGUAGE\", \"DBpedia\": [\"Language\"]},\n",
        "    {\"OntoNotes\": \"LAW\", \"DBpedia\": [\"TopicalConcept\", \"Work\"]},\n",
        "    {\"OntoNotes\": \"LOC\", \"DBpedia\": [\"Place\", \"TopicalConcept\"]},\n",
        "    {\"OntoNotes\": \"MONEY\", \"DBpedia\": [\"Currency\"]},\n",
        "    {\"OntoNotes\": \"NORP\", \"DBpedia\": [\"EthnicGroup\", \"Agent\"]},\n",
        "    {\"OntoNotes\": \"ORDINAL\", \"DBpedia\": [\"Identifier\"]},\n",
        "    {\"OntoNotes\": \"ORG\", \"DBpedia\": [\"Agent\", \"PersonFunction\"]},\n",
        "    {\"OntoNotes\": \"PERCENT\", \"DBpedia\": [\"UnitOfWork\"]},\n",
        "    {\"OntoNotes\": \"PERSON\", \"DBpedia\": [\"Agent\", \"PersonFunction\"]},\n",
        "    {\"OntoNotes\": \"PRODUCT\", \"DBpedia\": [\"Device\", \"Work\", \"MeanOfTransportation\"]},\n",
        "    {\"OntoNotes\": \"QUANTITY\", \"DBpedia\": [\"UnitOfWork\", \"Identifier\"]},\n",
        "    {\"OntoNotes\": \"TIME\", \"DBpedia\": [\"TimePeriod\"]},\n",
        "    {\"OntoNotes\": \"WORK_OF_ART\", \"DBpedia\": [\"Work\", \"Award\"]},\n",
        "\n",
        "    # Including all DBpedia classes explicitly\n",
        "    {\"OntoNotes\": \"\", \"DBpedia\": \"Species\"},\n",
        "    {\"OntoNotes\": \"\", \"DBpedia\": \"SportsSeason\"},\n",
        "    {\"OntoNotes\": \"\", \"DBpedia\": \"ChemicalSubstance\"},\n",
        "    {\"OntoNotes\": \"\", \"DBpedia\": \"Biomolecule\"},\n",
        "    {\"OntoNotes\": \"\", \"DBpedia\": \"Disease\"},\n",
        "    {\"OntoNotes\": \"\", \"DBpedia\": \"Food\"},\n",
        "    {\"OntoNotes\": \"\", \"DBpedia\": \"AnatomicalStructure\"},\n",
        "    {\"OntoNotes\": \"\", \"DBpedia\": \"Name\"},\n",
        "    {\"OntoNotes\": \"\", \"DBpedia\": \"Colour\"},\n",
        "    {\"OntoNotes\": \"\", \"DBpedia\": \"Pandemic\"},\n",
        "    {\"OntoNotes\": \"\", \"DBpedia\": \"SportCompetitionResult\"},\n",
        "    {\"OntoNotes\": \"\", \"DBpedia\": \"MedicalSpecialty\"}\n",
        "]\n"
      ],
      "metadata": {
        "id": "q9ThLepmwAha"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DBPEDIA_LOOKUP_ENDPOINT = \"https://lookup.dbpedia.org/api/search\""
      ],
      "metadata": {
        "id": "iv2oRlx9qUS1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def search_dbpedia(entity_text, dbpedia_type=None, max_results=3):\n",
        "    \"\"\"\n",
        "    Query DBpedia Lookup API to retrieve information about an entity based on DBpedia type.\n",
        "    \"\"\"\n",
        "    params = {\n",
        "        \"query\": entity_text,\n",
        "        \"format\": \"JSON\",\n",
        "        \"maxResults\": max_results,\n",
        "    }\n",
        "    if dbpedia_type:\n",
        "        params[\"typeName\"] = dbpedia_type\n",
        "        params[\"typeNameRequired\"] = \"true\"\n",
        "\n",
        "    best_result = None\n",
        "    highest_score = float('-inf')\n",
        "\n",
        "    try:\n",
        "        response = requests.get(DBPEDIA_LOOKUP_ENDPOINT, params=params)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "\n",
        "        if data.get('docs'):\n",
        "            for doc in data['docs']:\n",
        "                score = float(doc.get('score', [0])[0])\n",
        "                if score > highest_score:\n",
        "                    highest_score = score\n",
        "                    best_result = {\n",
        "                        \"Label\": doc.get('label', ['Unknown'])[0].replace('<B>', '').replace('</B>', ''),\n",
        "                        \"URI\": doc.get('resource', ['Unknown'])[0],\n",
        "                        \"Description\": doc.get('comment', ['No description available'])[0].replace('<B>', '').replace('</B>', ''),\n",
        "                        \"score\": highest_score\n",
        "                    }\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error querying DBpedia for type {dbpedia_type}: {e}\")\n",
        "\n",
        "    return best_result if best_result else {\"Label\": \"No match found\", \"URI\": \"\", \"Description\": \"No description available\", \"score\": 0}\n"
      ],
      "metadata": {
        "id": "CgohVKfjy6z1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The following entities were disambiguated using DBpedia Lookup:\")\n",
        "for span in ner_spans:\n",
        "    entity_text = span[\"text\"]\n",
        "    entity_type = span[\"type\"]\n",
        "\n",
        "    # Find the DBpedia classes mapped to the OntoNotes class\n",
        "    dbpedia_classes = next(\n",
        "        (mapping[\"DBpedia\"] for mapping in ontonotes_to_dbpedia_mapping if mapping[\"OntoNotes\"] == entity_type),\n",
        "        []\n",
        "    )\n",
        "\n",
        "    best_result = None\n",
        "    best_score = -1\n",
        "\n",
        "    # Query DBpedia for each mapped class\n",
        "    for dbpedia_class in dbpedia_classes:\n",
        "        dbpedia_result = search_dbpedia(entity_text, dbpedia_type=dbpedia_class)\n",
        "\n",
        "        # Extract the score from the result\n",
        "        if dbpedia_result.get(\"score\"):\n",
        "            score = dbpedia_result[\"score\"]\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_result = dbpedia_result\n",
        "\n",
        "    # Process the best result\n",
        "    if best_result and best_result[\"Label\"] != \"No match found\":\n",
        "        print(f'\"{entity_text}\" → {best_result}')\n",
        "    else:\n",
        "        print(f'\"{entity_text}\" → No match found')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vzjSasb7qXY2",
        "outputId": "7d318e3d-e2e7-4b46-f24a-ca5ba7fabf1f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following entities were disambiguated using DBpedia Lookup:\n",
            "\"Notre Dame\" → {'Label': 'Notre-Dame de Paris', 'URI': 'http://dbpedia.org/resource/Notre-Dame_de_Paris', 'Description': 'Notre-Dame de Paris (; French: [nɔtʁə dam də paʁi] (); meaning \"Our Lady of Paris\"), referred to', 'score': 8339.885}\n",
            "\"Paris\" → {'Label': 'Paris', 'URI': 'http://dbpedia.org/resource/Paris', 'Description': 'Paris (French pronunciation: \\u200b[paʁi] ()) is the capital and most populous city of France, with a', 'score': 59193.582}\n",
            "\"five years\" → {'Label': '1905', 'URI': 'http://dbpedia.org/resource/1905', 'Description': '1905 (MCMV) was a common year starting on Sunday of the Gregorian calendar and a common year', 'score': 537.7726}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wikidata"
      ],
      "metadata": {
        "id": "iwySNhYqv1kF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n"
      ],
      "metadata": {
        "id": "L2Wzv6UYrJfU"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "WIKIDATA_SEARCH_ENDPOINT = \"https://www.wikidata.org/w/api.php\"\n",
        "WIKIDATA_GET_ENTITY_ENDPOINT = \"https://www.wikidata.org/w/api.php\""
      ],
      "metadata": {
        "id": "zOy_SWuV5cvO"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def search_wikidata(entity_text, max_results=3):\n",
        "    \"\"\"\n",
        "    Query Wikidata API to retrieve matching entities based on search text.\n",
        "    \"\"\"\n",
        "    params = {\n",
        "        \"action\": \"wbsearchentities\",\n",
        "        \"search\": entity_text,\n",
        "        \"format\": \"json\",\n",
        "        \"language\": \"en\",\n",
        "        \"uselang\": \"en\",\n",
        "        \"limit\": max_results,\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(WIKIDATA_SEARCH_ENDPOINT, params=params)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "\n",
        "        results = []\n",
        "        if data.get('search'):\n",
        "            for item in data['search']:\n",
        "                label = item['label']\n",
        "                description = item.get('description', 'No description available')\n",
        "                url = f\"https://www.wikidata.org/wiki/{item['id']}\"\n",
        "                entity_id = item['id']\n",
        "\n",
        "                # Now, we fetch detailed information about the entity to get its type (instance of)\n",
        "                type_info = get_entity_type(entity_id)\n",
        "\n",
        "                results.append({\n",
        "                    \"Label\": label,\n",
        "                    \"Description\": description,\n",
        "                    \"URL\": url,\n",
        "                    \"ID\": entity_id,\n",
        "                    \"Type\": type_info\n",
        "                })\n",
        "\n",
        "        return results\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error querying Wikidata: {e}\")\n",
        "        return []\n"
      ],
      "metadata": {
        "id": "Pag5T6jb5dug"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_entity_type(entity_id):\n",
        "    \"\"\"\n",
        "    Fetches the type of an entity (e.g., Person, Organisation) based on its 'instance of' property.\n",
        "    \"\"\"\n",
        "    params = {\n",
        "        \"action\": \"wbgetentities\",\n",
        "        \"ids\": entity_id,\n",
        "        \"sites\": \"wikidata\",\n",
        "        \"props\": \"claims\",\n",
        "        \"format\": \"json\",\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(WIKIDATA_GET_ENTITY_ENDPOINT, params=params)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "\n",
        "        # Check for the 'instance of' (P31) claim to determine the type of the entity\n",
        "        if \"entities\" in data and entity_id in data[\"entities\"]:\n",
        "            entity = data[\"entities\"][entity_id]\n",
        "            claims = entity.get(\"claims\", {})\n",
        "            if \"P31\" in claims:\n",
        "                # 'P31' is the property for \"instance of\", which typically identifies the entity's type\n",
        "                entity_type = claims[\"P31\"][0][\"mainsnak\"][\"datavalue\"][\"value\"][\"id\"]\n",
        "                # Return the type label from the corresponding Wikidata entity\n",
        "                type_label = get_entity_label(entity_type)\n",
        "                return type_label\n",
        "\n",
        "        return \"Unknown\"\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching entity type for {entity_id}: {e}\")\n",
        "        return \"Unknown\"\n"
      ],
      "metadata": {
        "id": "-3-DYQsj7-b4"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_similarity(entity_text, candidates):\n",
        "    \"\"\"\n",
        "    Compute cosine similarity between the input text and the candidates' label and description.\n",
        "    \"\"\"\n",
        "    documents = [entity_text]  # The entity_text is the query sentence\n",
        "    for candidate in candidates:\n",
        "        documents.append(candidate['Label'] + \" \" + candidate['Description'])\n",
        "\n",
        "    # Use TF-IDF Vectorizer to convert text to vectors\n",
        "    vectorizer = TfidfVectorizer(stop_words='english')\n",
        "    tfidf_matrix = vectorizer.fit_transform(documents)\n",
        "\n",
        "    # Compute cosine similarity between the input text (first row) and candidates (remaining rows)\n",
        "    cosine_similarities = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:])\n",
        "\n",
        "    # Assign similarity score to each candidate\n",
        "    for idx, candidate in enumerate(candidates):\n",
        "        candidate['Similarity'] = cosine_similarities[0][idx]\n",
        "\n",
        "    return sorted(candidates, key=lambda x: x['Similarity'], reverse=True)"
      ],
      "metadata": {
        "id": "mtvLok2x5gSZ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_entity_label(entity_id):\n",
        "    \"\"\"\n",
        "    Fetch the label of a Wikidata entity based on its ID.\n",
        "    \"\"\"\n",
        "    params = {\n",
        "        \"action\": \"wbgetentities\",\n",
        "        \"ids\": entity_id,\n",
        "        \"format\": \"json\",\n",
        "        \"props\": \"labels\",\n",
        "        \"languages\": \"en\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(WIKIDATA_GET_ENTITY_ENDPOINT, params=params)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "\n",
        "        if \"entities\" in data and entity_id in data[\"entities\"]:\n",
        "            return data[\"entities\"][entity_id][\"labels\"][\"en\"][\"value\"]\n",
        "\n",
        "        return \"Unknown\"\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching label for entity type {entity_id}: {e}\")\n",
        "        return \"Unknown\"\n",
        "\n",
        "# NER output from Flair (mocked here for demonstration purposes)\n",
        "sentence = \"Notre Dame, the iconic medieval cathedral in Paris, reopens after five years of speedy reconstruction work.\"\n",
        "ner_spans = [\n",
        "    {\"text\": \"Notre Dame\", \"type\": \"FAC\", \"score\": 1.0000},\n",
        "    {\"text\": \"Paris\", \"type\": \"GPE\", \"score\": 1.0000},\n",
        "    {\"text\": \"five years\", \"type\": \"DATE\", \"score\": 1.0000},\n",
        "]\n"
      ],
      "metadata": {
        "id": "VYJ7YHUx5kth"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process each entity in the NER output\n",
        "for span in ner_spans:\n",
        "    entity_text = span[\"text\"]\n",
        "    print(f\"Disambiguating entity: {entity_text}\")\n",
        "    results = search_wikidata(entity_text)\n",
        "\n",
        "    if results:\n",
        "        for result in results:\n",
        "            print(f\"Best match for '{entity_text}':\")\n",
        "            print(f\"Label: {result['Label']}\")\n",
        "            print(f\"Description: {result['Description']}\")\n",
        "            print(f\"URL: {result['URL']}\")\n",
        "            print(f\"Type: {result['Type']}\")\n",
        "            print(\"\\n\")\n",
        "    else:\n",
        "        print(f\"No matches found for '{entity_text}'\\n\")\n",
        "    print(\"----------\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Pj59Cei5n-n",
        "outputId": "d5610fe0-7cce-4603-b443-425c648ccd0a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Disambiguating entity: Notre Dame\n",
            "Best match for 'Notre Dame':\n",
            "Label: Mary\n",
            "Description: mother of Jesus\n",
            "URL: https://www.wikidata.org/wiki/Q345\n",
            "Type: human biblical figure\n",
            "\n",
            "\n",
            "Best match for 'Notre Dame':\n",
            "Label: University of Notre Dame\n",
            "Description: Catholic university located in South Bend, Indiana, United States\n",
            "URL: https://www.wikidata.org/wiki/Q178848\n",
            "Type: private university\n",
            "\n",
            "\n",
            "Best match for 'Notre Dame':\n",
            "Label: Notre-Dame de Paris\n",
            "Description: cathedral in Paris\n",
            "URL: https://www.wikidata.org/wiki/Q2981\n",
            "Type: Catholic cathedral\n",
            "\n",
            "\n",
            "----------\n",
            "\n",
            "Disambiguating entity: Paris\n",
            "Best match for 'Paris':\n",
            "Label: Paris\n",
            "Description: capital city of France\n",
            "URL: https://www.wikidata.org/wiki/Q90\n",
            "Type: commune of France\n",
            "\n",
            "\n",
            "Best match for 'Paris':\n",
            "Label: Paris\n",
            "Description: family name\n",
            "URL: https://www.wikidata.org/wiki/Q18331346\n",
            "Type: family name\n",
            "\n",
            "\n",
            "Best match for 'Paris':\n",
            "Label: Paris\n",
            "Description: unisex given name\n",
            "URL: https://www.wikidata.org/wiki/Q1158980\n",
            "Type: unisex given name\n",
            "\n",
            "\n",
            "----------\n",
            "\n",
            "Disambiguating entity: five years\n",
            "Best match for 'five years':\n",
            "Label: Five Years\n",
            "Description: David Bowie song\n",
            "URL: https://www.wikidata.org/wiki/Q5456203\n",
            "Type: musical work/composition\n",
            "\n",
            "\n",
            "Best match for 'five years':\n",
            "Label: Five Years\n",
            "Description: 1088th strip of the webcomic xkcd\n",
            "URL: https://www.wikidata.org/wiki/Q18615007\n",
            "Type: comic strip\n",
            "\n",
            "\n",
            "Best match for 'five years':\n",
            "Label: Five Years (1969–1973)\n",
            "Description: 2015 compilation box set by David Bowie\n",
            "URL: https://www.wikidata.org/wiki/Q20656925\n",
            "Type: album\n",
            "\n",
            "\n",
            "----------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4OF-949j8F6J"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "737qeRLp6bhm"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}